{
  
    
        "post0": {
            "title": "Assigment 2",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import pydotplus import seaborn as sns from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report from sklearn.model_selection import cross_val_score from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image . Task 1: Data Retrieving . # Dataset can be found at https://www.kaggle.com/karangadiya/fifa19 # Dataset is also included in assignment zip file df = pd.read_csv(&#39;fifa_full.csv&#39;, sep=&#39;,&#39;) . Access Dataset Here . pd.set_option(&#39;display.max_columns&#39;, None) # Preview the dataframe df.head() . Unnamed: 0 ID Name Age Photo Nationality Flag Overall Potential Club Club Logo Value Wage Special Preferred Foot International Reputation Weak Foot Skill Moves Work Rate Body Type Real Face Position Jersey Number Joined Loaned From Contract Valid Until Height Weight LS ST RS LW LF CF RF RW LAM CAM RAM LM LCM CM RCM RM LWB LDM CDM RDM RWB LB LCB CB RCB RB Crossing Finishing HeadingAccuracy ShortPassing Volleys Dribbling Curve FKAccuracy LongPassing BallControl Acceleration SprintSpeed Agility Reactions Balance ShotPower Jumping Stamina Strength LongShots Aggression Interceptions Positioning Vision Penalties Composure Marking StandingTackle SlidingTackle GKDiving GKHandling GKKicking GKPositioning GKReflexes Release Clause . 0 0 | 158023 | L. Messi | 31 | https://cdn.sofifa.org/players/4/19/158023.png | Argentina | https://cdn.sofifa.org/flags/52.png | 94 | 94 | FC Barcelona | https://cdn.sofifa.org/teams/2/light/241.png | €110.5M | €565K | 2202 | Left | 5.0 | 4.0 | 4.0 | Medium/ Medium | Messi | Yes | RF | 10.0 | Jul 1, 2004 | NaN | 2021 | 5&#39;7 | 159lbs | 88+2 | 88+2 | 88+2 | 92+2 | 93+2 | 93+2 | 93+2 | 92+2 | 93+2 | 93+2 | 93+2 | 91+2 | 84+2 | 84+2 | 84+2 | 91+2 | 64+2 | 61+2 | 61+2 | 61+2 | 64+2 | 59+2 | 47+2 | 47+2 | 47+2 | 59+2 | 84.0 | 95.0 | 70.0 | 90.0 | 86.0 | 97.0 | 93.0 | 94.0 | 87.0 | 96.0 | 91.0 | 86.0 | 91.0 | 95.0 | 95.0 | 85.0 | 68.0 | 72.0 | 59.0 | 94.0 | 48.0 | 22.0 | 94.0 | 94.0 | 75.0 | 96.0 | 33.0 | 28.0 | 26.0 | 6.0 | 11.0 | 15.0 | 14.0 | 8.0 | €226.5M | . 1 1 | 20801 | Cristiano Ronaldo | 33 | https://cdn.sofifa.org/players/4/19/20801.png | Portugal | https://cdn.sofifa.org/flags/38.png | 94 | 94 | Juventus | https://cdn.sofifa.org/teams/2/light/45.png | €77M | €405K | 2228 | Right | 5.0 | 4.0 | 5.0 | High/ Low | C. Ronaldo | Yes | ST | 7.0 | Jul 10, 2018 | NaN | 2022 | 6&#39;2 | 183lbs | 91+3 | 91+3 | 91+3 | 89+3 | 90+3 | 90+3 | 90+3 | 89+3 | 88+3 | 88+3 | 88+3 | 88+3 | 81+3 | 81+3 | 81+3 | 88+3 | 65+3 | 61+3 | 61+3 | 61+3 | 65+3 | 61+3 | 53+3 | 53+3 | 53+3 | 61+3 | 84.0 | 94.0 | 89.0 | 81.0 | 87.0 | 88.0 | 81.0 | 76.0 | 77.0 | 94.0 | 89.0 | 91.0 | 87.0 | 96.0 | 70.0 | 95.0 | 95.0 | 88.0 | 79.0 | 93.0 | 63.0 | 29.0 | 95.0 | 82.0 | 85.0 | 95.0 | 28.0 | 31.0 | 23.0 | 7.0 | 11.0 | 15.0 | 14.0 | 11.0 | €127.1M | . 2 2 | 190871 | Neymar Jr | 26 | https://cdn.sofifa.org/players/4/19/190871.png | Brazil | https://cdn.sofifa.org/flags/54.png | 92 | 93 | Paris Saint-Germain | https://cdn.sofifa.org/teams/2/light/73.png | €118.5M | €290K | 2143 | Right | 5.0 | 5.0 | 5.0 | High/ Medium | Neymar | Yes | LW | 10.0 | Aug 3, 2017 | NaN | 2022 | 5&#39;9 | 150lbs | 84+3 | 84+3 | 84+3 | 89+3 | 89+3 | 89+3 | 89+3 | 89+3 | 89+3 | 89+3 | 89+3 | 88+3 | 81+3 | 81+3 | 81+3 | 88+3 | 65+3 | 60+3 | 60+3 | 60+3 | 65+3 | 60+3 | 47+3 | 47+3 | 47+3 | 60+3 | 79.0 | 87.0 | 62.0 | 84.0 | 84.0 | 96.0 | 88.0 | 87.0 | 78.0 | 95.0 | 94.0 | 90.0 | 96.0 | 94.0 | 84.0 | 80.0 | 61.0 | 81.0 | 49.0 | 82.0 | 56.0 | 36.0 | 89.0 | 87.0 | 81.0 | 94.0 | 27.0 | 24.0 | 33.0 | 9.0 | 9.0 | 15.0 | 15.0 | 11.0 | €228.1M | . 3 3 | 193080 | De Gea | 27 | https://cdn.sofifa.org/players/4/19/193080.png | Spain | https://cdn.sofifa.org/flags/45.png | 91 | 93 | Manchester United | https://cdn.sofifa.org/teams/2/light/11.png | €72M | €260K | 1471 | Right | 4.0 | 3.0 | 1.0 | Medium/ Medium | Lean | Yes | GK | 1.0 | Jul 1, 2011 | NaN | 2020 | 6&#39;4 | 168lbs | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 17.0 | 13.0 | 21.0 | 50.0 | 13.0 | 18.0 | 21.0 | 19.0 | 51.0 | 42.0 | 57.0 | 58.0 | 60.0 | 90.0 | 43.0 | 31.0 | 67.0 | 43.0 | 64.0 | 12.0 | 38.0 | 30.0 | 12.0 | 68.0 | 40.0 | 68.0 | 15.0 | 21.0 | 13.0 | 90.0 | 85.0 | 87.0 | 88.0 | 94.0 | €138.6M | . 4 4 | 192985 | K. De Bruyne | 27 | https://cdn.sofifa.org/players/4/19/192985.png | Belgium | https://cdn.sofifa.org/flags/7.png | 91 | 92 | Manchester City | https://cdn.sofifa.org/teams/2/light/10.png | €102M | €355K | 2281 | Right | 4.0 | 5.0 | 4.0 | High/ High | Normal | Yes | RCM | 7.0 | Aug 30, 2015 | NaN | 2023 | 5&#39;11 | 154lbs | 82+3 | 82+3 | 82+3 | 87+3 | 87+3 | 87+3 | 87+3 | 87+3 | 88+3 | 88+3 | 88+3 | 88+3 | 87+3 | 87+3 | 87+3 | 88+3 | 77+3 | 77+3 | 77+3 | 77+3 | 77+3 | 73+3 | 66+3 | 66+3 | 66+3 | 73+3 | 93.0 | 82.0 | 55.0 | 92.0 | 82.0 | 86.0 | 85.0 | 83.0 | 91.0 | 91.0 | 78.0 | 76.0 | 79.0 | 91.0 | 77.0 | 91.0 | 63.0 | 90.0 | 75.0 | 91.0 | 76.0 | 61.0 | 87.0 | 94.0 | 79.0 | 88.0 | 68.0 | 58.0 | 51.0 | 15.0 | 13.0 | 5.0 | 10.0 | 13.0 | €196.4M | . # Drop unwanted columns for the purpose of this application # Note we dropped the Goalkeeping skills attributes, this would make it too easy for our model. # Plus in a real world scenario, these skills would be hard to measure for ALL players. df = df.drop([&#39;Unnamed: 0&#39;, &#39;Age&#39;, &#39;Photo&#39;, &#39;Nationality&#39;, &#39;Flag&#39;, &#39;Overall&#39;, &#39;Potential&#39;, &#39;Club&#39;, &#39;Club Logo&#39;, &#39;Value&#39;, &#39;Wage&#39;,&#39;Special&#39;, &#39;Preferred Foot&#39;, &#39;International Reputation&#39;, &#39;Weak Foot&#39;, &#39;Skill Moves&#39;, &#39;Work Rate&#39;, &#39;Body Type&#39;, &#39;Real Face&#39;, &#39;Jersey Number&#39;, &#39;Joined&#39;, &#39;Loaned From&#39;, &#39;Contract Valid Until&#39;, &#39;Height&#39;, &#39;Weight&#39;, &#39;LS&#39;, &#39;ST&#39;, &#39;RS&#39;, &#39;LW&#39;, &#39;LF&#39;, &#39;CF&#39;, &#39;RF&#39;, &#39;RW&#39;, &#39;LAM&#39;, &#39;CAM&#39;, &#39;RAM&#39;, &#39;LM&#39;, &#39;LCM&#39;, &#39;CM&#39;, &#39;RCM&#39;, &#39;RM&#39;, &#39;LWB&#39;, &#39;LDM&#39;, &#39;CDM&#39;, &#39;RDM&#39;, &#39;RWB&#39;, &#39;LB&#39;, &#39;LCB&#39;, &#39;CB&#39;, &#39;RCB&#39;, &#39;RB&#39;, &#39;GKDiving&#39;, &#39;GKHandling&#39;, &#39;GKKicking&#39;, &#39;GKPositioning&#39;, &#39;GKReflexes&#39;, &#39;Release Clause&#39;], axis = 1) . df.head() . ID Name Position Crossing Finishing HeadingAccuracy ShortPassing Volleys Dribbling Curve FKAccuracy LongPassing BallControl Acceleration SprintSpeed Agility Reactions Balance ShotPower Jumping Stamina Strength LongShots Aggression Interceptions Positioning Vision Penalties Composure Marking StandingTackle SlidingTackle . 0 158023 | L. Messi | RF | 84.0 | 95.0 | 70.0 | 90.0 | 86.0 | 97.0 | 93.0 | 94.0 | 87.0 | 96.0 | 91.0 | 86.0 | 91.0 | 95.0 | 95.0 | 85.0 | 68.0 | 72.0 | 59.0 | 94.0 | 48.0 | 22.0 | 94.0 | 94.0 | 75.0 | 96.0 | 33.0 | 28.0 | 26.0 | . 1 20801 | Cristiano Ronaldo | ST | 84.0 | 94.0 | 89.0 | 81.0 | 87.0 | 88.0 | 81.0 | 76.0 | 77.0 | 94.0 | 89.0 | 91.0 | 87.0 | 96.0 | 70.0 | 95.0 | 95.0 | 88.0 | 79.0 | 93.0 | 63.0 | 29.0 | 95.0 | 82.0 | 85.0 | 95.0 | 28.0 | 31.0 | 23.0 | . 2 190871 | Neymar Jr | LW | 79.0 | 87.0 | 62.0 | 84.0 | 84.0 | 96.0 | 88.0 | 87.0 | 78.0 | 95.0 | 94.0 | 90.0 | 96.0 | 94.0 | 84.0 | 80.0 | 61.0 | 81.0 | 49.0 | 82.0 | 56.0 | 36.0 | 89.0 | 87.0 | 81.0 | 94.0 | 27.0 | 24.0 | 33.0 | . 3 193080 | De Gea | GK | 17.0 | 13.0 | 21.0 | 50.0 | 13.0 | 18.0 | 21.0 | 19.0 | 51.0 | 42.0 | 57.0 | 58.0 | 60.0 | 90.0 | 43.0 | 31.0 | 67.0 | 43.0 | 64.0 | 12.0 | 38.0 | 30.0 | 12.0 | 68.0 | 40.0 | 68.0 | 15.0 | 21.0 | 13.0 | . 4 192985 | K. De Bruyne | RCM | 93.0 | 82.0 | 55.0 | 92.0 | 82.0 | 86.0 | 85.0 | 83.0 | 91.0 | 91.0 | 78.0 | 76.0 | 79.0 | 91.0 | 77.0 | 91.0 | 63.0 | 90.0 | 75.0 | 91.0 | 76.0 | 61.0 | 87.0 | 94.0 | 79.0 | 88.0 | 68.0 | 58.0 | 51.0 | . df.shape . (18207, 32) . df.isnull().sum() . ID 0 Name 0 Position 60 Crossing 48 Finishing 48 HeadingAccuracy 48 ShortPassing 48 Volleys 48 Dribbling 48 Curve 48 FKAccuracy 48 LongPassing 48 BallControl 48 Acceleration 48 SprintSpeed 48 Agility 48 Reactions 48 Balance 48 ShotPower 48 Jumping 48 Stamina 48 Strength 48 LongShots 48 Aggression 48 Interceptions 48 Positioning 48 Vision 48 Penalties 48 Composure 48 Marking 48 StandingTackle 48 SlidingTackle 48 dtype: int64 . # Because we already have over 18,000 players in the dataset, we will just delete them df = df.dropna() df.isnull().sum() . ID 0 Name 0 Position 0 Crossing 0 Finishing 0 HeadingAccuracy 0 ShortPassing 0 Volleys 0 Dribbling 0 Curve 0 FKAccuracy 0 LongPassing 0 BallControl 0 Acceleration 0 SprintSpeed 0 Agility 0 Reactions 0 Balance 0 ShotPower 0 Jumping 0 Stamina 0 Strength 0 LongShots 0 Aggression 0 Interceptions 0 Positioning 0 Vision 0 Penalties 0 Composure 0 Marking 0 StandingTackle 0 SlidingTackle 0 dtype: int64 . df.Position.value_counts() . ST 2152 GK 2025 CB 1778 CM 1394 LB 1322 RB 1291 RM 1124 LM 1095 CAM 958 CDM 948 RCB 662 LCB 648 LCM 395 RCM 391 LW 381 RW 370 RDM 248 LDM 243 LS 207 RS 203 RWB 87 LWB 78 CF 74 RAM 21 LAM 21 RF 16 LF 15 Name: Position, dtype: int64 . att = dict.fromkeys([&#39;ST&#39;, &#39;LW&#39;, &#39;RW&#39;, &#39;LS&#39;, &#39;RS&#39;, &#39;CF&#39;, &#39;RF&#39;, &#39;LF&#39;], &#39;Attacker&#39;) mid = dict.fromkeys([&#39;CM&#39;, &#39;RM&#39;, &#39;LM&#39;, &#39;CAM&#39;, &#39;CDM&#39;, &#39;LCM&#39;, &#39;RCM&#39;, &#39;RDM&#39;, &#39;LDM&#39;, &#39;RAM&#39;, &#39;LAM&#39;], &#39;Midfielder&#39;) dfnc = dict.fromkeys([&#39;CB&#39;, &#39;LB&#39;, &#39;RB&#39;, &#39;RCB&#39;, &#39;LCB&#39;, &#39;RWB&#39;, &#39;LWB&#39; ], &#39;Defender&#39;) df.Position.replace(&#39;GK&#39;, &#39;Goalkeeper&#39;, inplace=True) df.Position.replace(att, inplace=True) df.Position.replace(mid, inplace=True) df.Position.replace(dfnc, inplace=True) # Confirm the changes df.Position.value_counts() . Midfielder 6838 Defender 5866 Attacker 3418 Goalkeeper 2025 Name: Position, dtype: int64 . df.duplicated(&#39;ID&#39;).sum() . 0 . Task 2: Data Exploration . df.Position.value_counts().plot(kind = &#39;pie&#39;, autopct = &#39;%0.1f%%&#39;, shadow = True, cmap = &#39;Set3&#39; ) plt.title(&#39;Position Representation n&#39;, fontsize = 16 ) plt.xlabel(&#39;&#39;) plt.ylabel(&#39;&#39;) plt.axis(&#39;equal&#39;) plt.show() . # Each of these columns should range between 0 and 100. # The histogram and the min, max, mean values should provide us a quick way to validate the data. # Use a for loop to cycle through the attributes for column in df.iloc[:,3:]: sns.distplot(df[column], kde = False, bins = 20) plt.title(str(df[column].name) + &quot; Histogram&quot;, fontsize = 14) plt.ylabel(&quot;Count&quot;) plt.show() print df[column].name print df[column].describe() print &quot; n n&quot; . Crossing count 18147.000000 mean 49.738414 std 18.364255 min 5.000000 25% 38.000000 50% 54.000000 75% 64.000000 max 93.000000 Name: Crossing, dtype: float64 . Finishing count 18147.000000 mean 45.550229 std 19.527445 min 2.000000 25% 30.000000 50% 49.000000 75% 62.000000 max 95.000000 Name: Finishing, dtype: float64 . HeadingAccuracy count 18147.000000 mean 52.300766 std 17.381753 min 4.000000 25% 44.000000 50% 56.000000 75% 64.000000 max 94.000000 Name: HeadingAccuracy, dtype: float64 . ShortPassing count 18147.000000 mean 58.695432 std 14.696075 min 7.000000 25% 54.000000 50% 62.000000 75% 68.000000 max 93.000000 Name: ShortPassing, dtype: float64 . Volleys count 18147.000000 mean 42.912217 std 17.695900 min 4.000000 25% 30.000000 50% 44.000000 75% 57.000000 max 90.000000 Name: Volleys, dtype: float64 . Dribbling count 18147.000000 mean 55.375158 std 18.912224 min 4.000000 25% 49.000000 50% 61.000000 75% 68.000000 max 97.000000 Name: Dribbling, dtype: float64 . Curve count 18147.000000 mean 47.176283 std 18.396009 min 6.000000 25% 34.000000 50% 48.000000 75% 62.000000 max 94.000000 Name: Curve, dtype: float64 . FKAccuracy count 18147.000000 mean 42.866038 std 17.480034 min 3.000000 25% 31.000000 50% 41.000000 75% 57.000000 max 94.000000 Name: FKAccuracy, dtype: float64 . LongPassing count 18147.000000 mean 52.721386 std 15.325211 min 9.000000 25% 43.000000 50% 56.000000 75% 64.000000 max 93.000000 Name: LongPassing, dtype: float64 . BallControl count 18147.000000 mean 58.374828 std 16.685643 min 5.000000 25% 54.000000 50% 63.000000 75% 69.000000 max 96.000000 Name: BallControl, dtype: float64 . Acceleration count 18147.000000 mean 64.612829 std 14.930320 min 12.000000 25% 57.000000 50% 67.000000 75% 75.000000 max 97.000000 Name: Acceleration, dtype: float64 . SprintSpeed count 18147.000000 mean 64.726236 std 14.651776 min 12.000000 25% 57.000000 50% 67.000000 75% 75.000000 max 96.000000 Name: SprintSpeed, dtype: float64 . Agility count 18147.000000 mean 63.501295 std 14.768956 min 14.000000 25% 55.000000 50% 66.000000 75% 74.000000 max 96.000000 Name: Agility, dtype: float64 . Reactions count 18147.000000 mean 61.839147 std 9.011056 min 21.000000 25% 56.000000 50% 62.000000 75% 68.000000 max 96.000000 Name: Reactions, dtype: float64 . Balance count 18147.000000 mean 63.964292 std 14.136073 min 16.000000 25% 56.000000 50% 66.000000 75% 74.000000 max 96.000000 Name: Balance, dtype: float64 . ShotPower count 18147.000000 mean 55.465201 std 17.235534 min 2.000000 25% 45.000000 50% 59.000000 75% 68.000000 max 95.000000 Name: ShotPower, dtype: float64 . Jumping count 18147.000000 mean 65.091034 std 11.822327 min 15.000000 25% 58.000000 50% 66.000000 75% 73.000000 max 95.000000 Name: Jumping, dtype: float64 . Stamina count 18147.000000 mean 63.221579 std 15.896381 min 12.000000 25% 56.000000 50% 66.000000 75% 74.000000 max 96.000000 Name: Stamina, dtype: float64 . Strength count 18147.000000 mean 65.318620 std 12.552479 min 17.000000 25% 58.000000 50% 67.000000 75% 74.000000 max 97.000000 Name: Strength, dtype: float64 . LongShots count 18147.000000 mean 47.113187 std 19.263142 min 3.000000 25% 33.000000 50% 51.000000 75% 62.000000 max 94.000000 Name: LongShots, dtype: float64 . Aggression count 18147.000000 mean 55.876068 std 17.366534 min 11.000000 25% 44.000000 50% 59.000000 75% 69.000000 max 95.000000 Name: Aggression, dtype: float64 . Interceptions count 18147.000000 mean 46.702761 std 20.697462 min 3.000000 25% 26.000000 50% 52.000000 75% 64.000000 max 92.000000 Name: Interceptions, dtype: float64 . Positioning count 18147.000000 mean 49.962198 std 19.530469 min 2.000000 25% 38.000000 50% 55.000000 75% 64.000000 max 95.000000 Name: Positioning, dtype: float64 . Vision count 18147.000000 mean 53.407781 std 14.146594 min 10.000000 25% 44.000000 50% 55.000000 75% 64.000000 max 94.000000 Name: Vision, dtype: float64 . Penalties count 18147.000000 mean 48.546371 std 15.703113 min 5.000000 25% 39.000000 50% 49.000000 75% 60.000000 max 92.000000 Name: Penalties, dtype: float64 . Composure count 18147.000000 mean 58.651127 std 11.437138 min 3.000000 25% 51.000000 50% 60.000000 75% 67.000000 max 96.000000 Name: Composure, dtype: float64 . Marking count 18147.000000 mean 47.286053 std 19.900450 min 3.000000 25% 30.000000 50% 53.000000 75% 64.000000 max 94.000000 Name: Marking, dtype: float64 . StandingTackle count 18147.000000 mean 47.701879 std 21.663630 min 2.000000 25% 27.000000 50% 55.000000 75% 66.000000 max 93.000000 Name: StandingTackle, dtype: float64 . SlidingTackle count 18147.000000 mean 45.666336 std 21.287961 min 3.000000 25% 24.000000 50% 52.000000 75% 64.000000 max 91.000000 Name: SlidingTackle, dtype: float64 . sns.scatterplot(x = &quot;Reactions&quot;, y = &quot;Balance&quot;, hue = &quot;Position&quot;, data = df, ) plt.title(&#39;Balance Vs Reactions by Position&#39;, fontsize = 14) plt.show() . sns.scatterplot(x = &quot;Strength&quot;, y = &quot;Jumping&quot;, hue = &quot;Position&quot;, data = df, ) plt.title(&#39;Jumping Vs Strength by Position&#39;, fontsize = 14) plt.show() . sns.scatterplot(x = &quot;Finishing&quot;, y = &quot;HeadingAccuracy&quot;, hue = &quot;Position&quot;, data = df, ) plt.title(&#39;HeadingAccuracy Vs Finishing by Position&#39;, fontsize = 14) plt.show() . sns.scatterplot(x = &quot;Interceptions&quot;, y = &quot;Marking&quot;, hue = &quot;Position&quot;, data = df, ) plt.title(&#39;Interceptions Vs Marking by Position&#39;, fontsize = 14) plt.show() . sns.scatterplot(x = &quot;SlidingTackle&quot;, y = &quot;StandingTackle&quot;, hue = &quot;Position&quot;, data = df, ) plt.title(&#39;StandingTackle Vs SlidingTackle by Position&#39;, fontsize = 14) plt.show() . sns.scatterplot(x = &quot;SlidingTackle&quot;, y = &quot;Finishing&quot;, hue = &quot;Position&quot;, data = df, ) plt.title(&#39;Finishing Vs SlidingTackle by Position&#39;, fontsize = 14) plt.show() . # The box plots will show which Positions excel in each skill # Outliers removed for cleaner plots for column in df.iloc[:,3:]: sns.boxplot(x = &#39;Position&#39;, y = df[column], data = df, width = 0.2, fliersize = 0 ) plt.title(str(df[column].name) + &quot; by Position&quot;, fontsize = 14) plt.ylabel(&quot;Rating&quot;) plt.show() . # Plot the 4 skills in a kde matrix to try an identify clusters special_skills = df[[&#39;Finishing&#39;, &#39;HeadingAccuracy&#39;,&#39;SlidingTackle&#39;, &#39;StandingTackle&#39;]] g = sns.PairGrid(special_skills) g.map_diag(sns.kdeplot) g.map_offdiag(sns.kdeplot, n_levels = 20) plt.show() . # Plot the 4 skills in a kde matrix to try an identify clusters general_skills = df[[&#39;Balance&#39;, &#39;Reactions&#39;, &#39;Jumping&#39;, &#39;Strength&#39;]] g = sns.PairGrid(general_skills) g.map_diag(sns.kdeplot) g.map_offdiag(sns.kdeplot, n_levels = 20) plt.show() . Task 3: Data Modelling: Classification . df.dtypes . ID int64 Name object Position object Crossing float64 Finishing float64 HeadingAccuracy float64 ShortPassing float64 Volleys float64 Dribbling float64 Curve float64 FKAccuracy float64 LongPassing float64 BallControl float64 Acceleration float64 SprintSpeed float64 Agility float64 Reactions float64 Balance float64 ShotPower float64 Jumping float64 Stamina float64 Strength float64 LongShots float64 Aggression float64 Interceptions float64 Positioning float64 Vision float64 Penalties float64 Composure float64 Marking float64 StandingTackle float64 SlidingTackle float64 dtype: object . Decision Tree 1: 50% Training Set, 50% Test Set . Set the Features, Outcomes and Training/Test Set . feature_cols = [&#39;Crossing&#39;,&#39;Finishing&#39;,&#39;HeadingAccuracy&#39;,&#39;ShortPassing&#39;,&#39;Volleys&#39;,&#39;Dribbling&#39;,&#39;Curve&#39;,&#39;FKAccuracy&#39;, &#39;LongPassing&#39;,&#39;BallControl&#39;,&#39;Acceleration&#39;,&#39;SprintSpeed&#39;,&#39;Agility&#39;,&#39;Reactions&#39;,&#39;Balance&#39;,&#39;ShotPower&#39;, &#39;Jumping&#39;,&#39;Stamina&#39;,&#39;Strength&#39;,&#39;LongShots&#39;,&#39;Aggression&#39;,&#39;Interceptions&#39;,&#39;Positioning&#39;,&#39;Vision&#39;, &#39;Penalties&#39;,&#39;Composure&#39;,&#39;Marking&#39;,&#39;StandingTackle&#39;,&#39;SlidingTackle&#39;] # Assign the feature data X = df[feature_cols] # Assign the outcomes y = df.Position # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1984) . Identify the best max_depth value. Keep the max value at 5 as we want to minimise the nodes significantly . error_rate = [] # Set the range of potential max_depth # Run clf for each max_depth in the range for i in range(1,6): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = i, # Default None min_samples_split = 2, # Default 2 min_samples_leaf = 1 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a max_depth value plt.plot(range(1,6), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. max_depth&#39;) plt.xlabel(&#39;max_depth&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . max_depth chosen is 5 . Now check min_samples_split, we want the highest value for the lowest error to minimise overfitting . error_rate = [] # Set the range of potential min_samples_split # Run clf for each mmin_samples_split in the range for i in range(10,210,10): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = 5, # Default None min_samples_split = i, # Default 2 min_samples_leaf = 1 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a min_samples_split value plt.plot(range(10,210,10), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. min_samples_split&#39;) plt.xlabel(&#39;min_samples_split (x10)&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . min_sample_split chosen is 50 . Now check min_samples_leaf, we want the highest value for the lowest error to minimise overfitting . error_rate = [] # Set the range of potential min_samples_leaf # Run clf for each min_samples_leaf in the range for i in range(1,51): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = 5, # Default None min_samples_split = 50, # Default 2 min_samples_leaf = i # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a min_samples_leaf value plt.plot(range(1,51), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. min_samples_leaf&#39;) plt.xlabel(&#39;min_samples_leaf&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . min_sample_leaf chosen is 8 . Now check max_features, we want the lowest value for the lowest error . error_rate = [] # Set the range of potential max_features # Run clf for each max_features in the range for i in range(1,30): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = i, # Default None max_depth = 5, # Default None min_samples_split = 50, # Default 2 min_samples_leaf = 8 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a max_features value plt.plot(range(1,30), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. max_features&#39;) plt.xlabel(&#39;max_features&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . max_features chosen is None . Build the Decision Tree Classifier with the default parameters . clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = None, # Default None min_samples_split = 2, # Default 2 min_samples_leaf = 1 # Default 1 ) # Train the model on the training set fit = clf.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(clf, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; # Set the model node count to a variable nds = clf.tree_.node_count print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Number of Nodes in Model: &quot;,nds print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 80.85% Cross Validation Accuracy: 79.25% Model Fitting: 1.59% Overfitted Number of Nodes in Model: 1707 Error Rate: 19.15% Confusion Matrix: [[1220 20 0 446] [ 19 2516 0 412] [ 0 1 1002 0] [ 419 421 0 2598]] Classification Report: precision recall f1-score support Attacker 0.74 0.72 0.73 1686 Defender 0.85 0.85 0.85 2947 Goalkeeper 1.00 1.00 1.00 1003 Midfielder 0.75 0.76 0.75 3438 micro avg 0.81 0.81 0.81 9074 macro avg 0.83 0.83 0.83 9074 weighted avg 0.81 0.81 0.81 9074 . Now build the model with the tuned parameters . clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = 5, # Default None min_samples_split = 50, # Default 2 min_samples_leaf = 8 # Default 1 ) # Train the model on the training set fit = clf.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(clf, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; # Set the model node count to a variable nds = clf.tree_.node_count print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Number of Nodes in Model: &quot;,nds print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 82.36% Cross Validation Accuracy: 81.03% Model Fitting: 1.33% Overfitted Number of Nodes in Model: 51 Error Rate: 17.64% Confusion Matrix: [[1240 12 0 434] [ 11 2540 0 396] [ 0 1 1001 1] [ 283 463 0 2692]] Classification Report: precision recall f1-score support Attacker 0.81 0.74 0.77 1686 Defender 0.84 0.86 0.85 2947 Goalkeeper 1.00 1.00 1.00 1003 Midfielder 0.76 0.78 0.77 3438 micro avg 0.82 0.82 0.82 9074 macro avg 0.85 0.84 0.85 9074 weighted avg 0.82 0.82 0.82 9074 . Outcome: . Tuning the model improves performance in all areas as seen by the output . Decision Tree 2: 60% Training Set, 40% Test Set . Set the Features, Outcomes and Training/Test Set . feature_cols = [&#39;Crossing&#39;,&#39;Finishing&#39;,&#39;HeadingAccuracy&#39;,&#39;ShortPassing&#39;,&#39;Volleys&#39;,&#39;Dribbling&#39;,&#39;Curve&#39;,&#39;FKAccuracy&#39;, &#39;LongPassing&#39;,&#39;BallControl&#39;,&#39;Acceleration&#39;,&#39;SprintSpeed&#39;,&#39;Agility&#39;,&#39;Reactions&#39;,&#39;Balance&#39;,&#39;ShotPower&#39;, &#39;Jumping&#39;,&#39;Stamina&#39;,&#39;Strength&#39;,&#39;LongShots&#39;,&#39;Aggression&#39;,&#39;Interceptions&#39;,&#39;Positioning&#39;,&#39;Vision&#39;, &#39;Penalties&#39;,&#39;Composure&#39;,&#39;Marking&#39;,&#39;StandingTackle&#39;,&#39;SlidingTackle&#39;] # Assign the feature data X = df[feature_cols] # Assign the outcomes y = df.Position # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1984) . Identify the best max_depth value. Keep the max value at 5 as we want to minimise the nodes significantly . error_rate = [] # Set the range of potential max_depth # Run clf for each max_depth in the range for i in range(1,6): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = i, # Default None min_samples_split = 2, # Default 2 min_samples_leaf = 1 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a max_depth value plt.plot(range(1,6), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. max_depth&#39;) plt.xlabel(&#39;max_depth&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . max_depth chosen is 5 . Now check min_samples_split, we want the highest value for the lowest error to minimise overfitting . error_rate = [] # Set the range of potential min_samples_split # Run clf for each mmin_samples_split in the range for i in range(10,210,10): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = 5, # Default None min_samples_split = i, # Default 2 min_samples_leaf = 1 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a min_samples_split value plt.plot(range(10,210,10), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. min_samples_split&#39;) plt.xlabel(&#39;min_samples_split (x10)&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . min_sample_split chosen is 100 . Now check min_samples_leaf, we want the highest value for the lowest error to minimise overfitting . error_rate = [] # Set the range of potential min_samples_leaf # Run clf for each min_samples_leaf in the range for i in range(1,51): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = 5, # Default None min_samples_split = 100, # Default 2 min_samples_leaf = i # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a min_samples_leaf value plt.plot(range(1,51), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. min_samples_leaf&#39;) plt.xlabel(&#39;min_samples_leaf&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . min_sample_leaf chosen is 7 . Now check max_features, we want the lowest value for the lowest error . error_rate = [] # Set the range of potential max_features # Run clf for each max_features in the range for i in range(1,30): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = i, # Default None max_depth = 5, # Default None min_samples_split = 100, # Default 2 min_samples_leaf = 7 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a max_features value plt.plot(range(1,30), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. max_features&#39;) plt.xlabel(&#39;max_features&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . max_features chosen is 20 . Build the Decision Tree Classifier with the default parameters . clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = None, # Default None min_samples_split = 2, # Default 2 min_samples_leaf = 1 # Default 1 ) # Train the model on the training set fit = clf.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(clf, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; # Set the model node count to a variable nds = clf.tree_.node_count print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Number of Nodes in Model: &quot;,nds print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 80.7% Cross Validation Accuracy: 79.05% Model Fitting: 1.65% Overfitted Number of Nodes in Model: 2035 Error Rate: 19.3% Confusion Matrix: [[1023 9 0 335] [ 14 2002 0 310] [ 1 0 808 0] [ 395 337 0 2025]] Classification Report: precision recall f1-score support Attacker 0.71 0.75 0.73 1367 Defender 0.85 0.86 0.86 2326 Goalkeeper 1.00 1.00 1.00 809 Midfielder 0.76 0.73 0.75 2757 micro avg 0.81 0.81 0.81 7259 macro avg 0.83 0.84 0.83 7259 weighted avg 0.81 0.81 0.81 7259 . Now build the model with the tuned parameters . clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = 20, # Default None max_depth = 5, # Default None min_samples_split = 100, # Default 2 min_samples_leaf = 7 # Default 1 ) # Train the model on the training set fit = clf.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(clf, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; # Set the model node count to a variable nds = clf.tree_.node_count print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Number of Nodes in Model: &quot;,nds print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 82.01% Cross Validation Accuracy: 80.91% Model Fitting: 1.1% Overfitted Number of Nodes in Model: 51 Error Rate: 17.99% Confusion Matrix: [[1012 11 0 344] [ 8 2089 0 229] [ 0 1 807 1] [ 214 498 0 2045]] Classification Report: precision recall f1-score support Attacker 0.82 0.74 0.78 1367 Defender 0.80 0.90 0.85 2326 Goalkeeper 1.00 1.00 1.00 809 Midfielder 0.78 0.74 0.76 2757 micro avg 0.82 0.82 0.82 7259 macro avg 0.85 0.84 0.85 7259 weighted avg 0.82 0.82 0.82 7259 . Outcome: . Tuning the model improves performance in all areas as seen by the output . Decision Tree 3: 80% Training Set, 20% Test Set . Set the Features, Outcomes and Training/Test Set . feature_cols = [&#39;Crossing&#39;,&#39;Finishing&#39;,&#39;HeadingAccuracy&#39;,&#39;ShortPassing&#39;,&#39;Volleys&#39;,&#39;Dribbling&#39;,&#39;Curve&#39;,&#39;FKAccuracy&#39;, &#39;LongPassing&#39;,&#39;BallControl&#39;,&#39;Acceleration&#39;,&#39;SprintSpeed&#39;,&#39;Agility&#39;,&#39;Reactions&#39;,&#39;Balance&#39;,&#39;ShotPower&#39;, &#39;Jumping&#39;,&#39;Stamina&#39;,&#39;Strength&#39;,&#39;LongShots&#39;,&#39;Aggression&#39;,&#39;Interceptions&#39;,&#39;Positioning&#39;,&#39;Vision&#39;, &#39;Penalties&#39;,&#39;Composure&#39;,&#39;Marking&#39;,&#39;StandingTackle&#39;,&#39;SlidingTackle&#39;] # Assign the feature data X = df[feature_cols] # Assign the outcomes y = df.Position # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1984) . Identify the best max_depth value. Keep the max value at 5 as we want to minimise the nodes significantly . error_rate = [] # Set the range of potential max_depth # Run clf for each max_depth in the range for i in range(1,6): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = i, # Default None min_samples_split = 2, # Default 2 min_samples_leaf = 1 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a max_depth value plt.plot(range(1,6), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. max_depth&#39;) plt.xlabel(&#39;max_depth&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . max_depth chosen is 5 . Now check min_samples_split, we want the highest value for the lowest error to minimise overfitting . error_rate = [] # Set the range of potential min_samples_split # Run clf for each mmin_samples_split in the range for i in range(10,210,10): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = 5, # Default None min_samples_split = i, # Default 2 min_samples_leaf = 1 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a min_samples_split value plt.plot(range(10,210,10), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. min_samples_split&#39;) plt.xlabel(&#39;min_samples_split (x10)&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . min_sample_split chosen is 20 . Now check min_samples_leaf, we want the highest value for the lowest error to minimise overfitting . error_rate = [] # Set the range of potential min_samples_leaf # Run clf for each min_samples_leaf in the range for i in range(1,51): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = 5, # Default None min_samples_split = 20, # Default 2 min_samples_leaf = i # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a min_samples_leaf value plt.plot(range(1,51), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. min_samples_leaf&#39;) plt.xlabel(&#39;min_samples_leaf&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . min_sample_leaf chosen is 5 . Now check max_features, we want the lowest value for the lowest error . error_rate = [] # Set the range of potential max_features # Run clf for each max_features in the range for i in range(1,30): clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = i, # Default None max_depth = 5, # Default None min_samples_split = 20, # Default 2 min_samples_leaf = 5 # Default 1 ) fit = clf.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a max_features value plt.plot(range(1,30), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. max_features&#39;) plt.xlabel(&#39;max_features&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . max_features chosen is 23 . Build the Decision Tree Classifier with the default parameters . clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = None, # Default None max_depth = None, # Default None min_samples_split = 2, # Default 2 min_samples_leaf = 1 # Default 1 ) # Train the model on the training set fit = clf.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(clf, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; # Set the model node count to a variable nds = clf.tree_.node_count print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Number of Nodes in Model: &quot;,nds print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 82.01% Cross Validation Accuracy: 79.2% Model Fitting: 2.81% Overfitted Number of Nodes in Model: 2715 Error Rate: 17.99% Confusion Matrix: [[ 519 3 0 184] [ 5 1001 0 128] [ 0 0 402 0] [ 174 159 0 1055]] Classification Report: precision recall f1-score support Attacker 0.74 0.74 0.74 706 Defender 0.86 0.88 0.87 1134 Goalkeeper 1.00 1.00 1.00 402 Midfielder 0.77 0.76 0.77 1388 micro avg 0.82 0.82 0.82 3630 macro avg 0.84 0.84 0.84 3630 weighted avg 0.82 0.82 0.82 3630 . Now build the model with the tuned parameters . clf = DecisionTreeClassifier(criterion = &#39;gini&#39;, # Default gini max_features = 23, # Default None max_depth = 5, # Default None min_samples_split = 20, # Default 2 min_samples_leaf = 5 # Default 1 ) # Train the model on the training set fit = clf.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(clf, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; # Set the model node count to a variable nds = clf.tree_.node_count print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Number of Nodes in Model: &quot;,nds print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 82.42% Cross Validation Accuracy: 80.71% Model Fitting: 1.72% Overfitted Number of Nodes in Model: 51 Error Rate: 17.58% Confusion Matrix: [[ 523 8 0 175] [ 1 1046 0 87] [ 0 1 400 1] [ 106 259 0 1023]] Classification Report: precision recall f1-score support Attacker 0.83 0.74 0.78 706 Defender 0.80 0.92 0.85 1134 Goalkeeper 1.00 1.00 1.00 402 Midfielder 0.80 0.74 0.77 1388 micro avg 0.82 0.82 0.82 3630 macro avg 0.86 0.85 0.85 3630 weighted avg 0.83 0.82 0.82 3630 . Outcome: . Tuning the model improves performance in all areas as seen by the output . Decision Tree Classifier Outcome: . The best performing model was the one built on the 80% training set based on the output of each model. . dot_data = StringIO() export_graphviz(fit, out_file = dot_data, filled = True, rounded = True, special_characters = True, feature_names = feature_cols, class_names = [&#39;Attacker&#39;, &#39;Defender&#39;, &#39;Goalkeeper&#39;, &#39;Midfielder&#39;] ) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png(&#39;fifa-clf.png&#39;) Image(graph.create_png()) . KNN 1: 50% Training Set, 50% Test Set . Set the Features, Outcomes and Training/Test Set . feature_cols = [&#39;Crossing&#39;,&#39;Finishing&#39;,&#39;HeadingAccuracy&#39;,&#39;ShortPassing&#39;,&#39;Volleys&#39;,&#39;Dribbling&#39;,&#39;Curve&#39;,&#39;FKAccuracy&#39;, &#39;LongPassing&#39;,&#39;BallControl&#39;,&#39;Acceleration&#39;,&#39;SprintSpeed&#39;,&#39;Agility&#39;,&#39;Reactions&#39;,&#39;Balance&#39;,&#39;ShotPower&#39;, &#39;Jumping&#39;,&#39;Stamina&#39;,&#39;Strength&#39;,&#39;LongShots&#39;,&#39;Aggression&#39;,&#39;Interceptions&#39;,&#39;Positioning&#39;,&#39;Vision&#39;, &#39;Penalties&#39;,&#39;Composure&#39;,&#39;Marking&#39;,&#39;StandingTackle&#39;,&#39;SlidingTackle&#39;] # Assign the feature data X = df[feature_cols] # Assign the outcomes y = df.Position # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1984) . Use the Elbow Method to find the best K . We&#39;re looking for the lowest value that gives us the lowest error rate . # The elbow method plots the error rate of a range of K values error_rate = [] # Set the range of potential K values # Run KNN for each K in the range for i in range(1,32,2): knn = KNeighborsClassifier(n_neighbors = i) fit = knn.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a K value plt.plot(range(1,32,2), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. K Value&#39;) plt.xlabel(&#39;K&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . K value chosen is 7 . Now let&#39;s test the P value for 1 and 2 . error_rate = [] # Set the range of potential P values # Run KNN for each P in the range for i in range(1,3): knn = KNeighborsClassifier(n_neighbors = 7, p = i) fit = knn.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a P value plt.plot(range(1,3), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. P Value&#39;) plt.xlabel(&#39;P&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . P value chosen is 2 (Euclidean Distance) . Build the KNN with the default parameters . knn = KNeighborsClassifier(n_neighbors = 5, # Default 5 weights = &#39;uniform&#39;, # Default uniform metric = &#39;minkowski&#39;, # Default minkowski p = 2 # Default 2 ) # Train the model on the training set fit = knn.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(knn, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 86.13% Cross Validation Accuracy: 85.79% Model Fitting: 0.33% Overfitted Error Rate: 13.87% Confusion Matrix: [[1292 13 0 381] [ 3 2654 0 290] [ 0 0 1003 0] [ 305 267 0 2866]] Classification Report: precision recall f1-score support Attacker 0.81 0.77 0.79 1686 Defender 0.90 0.90 0.90 2947 Goalkeeper 1.00 1.00 1.00 1003 Midfielder 0.81 0.83 0.82 3438 micro avg 0.86 0.86 0.86 9074 macro avg 0.88 0.88 0.88 9074 weighted avg 0.86 0.86 0.86 9074 . Now build the KNN with the tuned parameters . knn = KNeighborsClassifier(n_neighbors = 7, # Default 5 weights = &#39;uniform&#39;, # Default uniform metric = &#39;minkowski&#39;, # Default minkowski p = 2 # Default 2 ) # Train the model on the training set fit = knn.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(knn, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 86.63% Cross Validation Accuracy: 86.48% Model Fitting: 0.15% Overfitted Error Rate: 13.37% Confusion Matrix: [[1290 12 0 384] [ 2 2661 0 284] [ 0 0 1002 1] [ 272 258 0 2908]] Classification Report: precision recall f1-score support Attacker 0.82 0.77 0.79 1686 Defender 0.91 0.90 0.91 2947 Goalkeeper 1.00 1.00 1.00 1003 Midfielder 0.81 0.85 0.83 3438 micro avg 0.87 0.87 0.87 9074 macro avg 0.89 0.88 0.88 9074 weighted avg 0.87 0.87 0.87 9074 . Outcome: . Tuning the model improves performance in all areas as seen by the output . KNN 2: 60% Training Set, 40% Test Set . Set the Features, Outcomes and Training/Test Set . feature_cols = [&#39;Crossing&#39;,&#39;Finishing&#39;,&#39;HeadingAccuracy&#39;,&#39;ShortPassing&#39;,&#39;Volleys&#39;,&#39;Dribbling&#39;,&#39;Curve&#39;,&#39;FKAccuracy&#39;, &#39;LongPassing&#39;,&#39;BallControl&#39;,&#39;Acceleration&#39;,&#39;SprintSpeed&#39;,&#39;Agility&#39;,&#39;Reactions&#39;,&#39;Balance&#39;,&#39;ShotPower&#39;, &#39;Jumping&#39;,&#39;Stamina&#39;,&#39;Strength&#39;,&#39;LongShots&#39;,&#39;Aggression&#39;,&#39;Interceptions&#39;,&#39;Positioning&#39;,&#39;Vision&#39;, &#39;Penalties&#39;,&#39;Composure&#39;,&#39;Marking&#39;,&#39;StandingTackle&#39;,&#39;SlidingTackle&#39;] # Assign the feature data X = df[feature_cols] # Assign the outcomes y = df.Position # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1984) . Use the Elbow Method to find the best K . We&#39;re looking for the lowest value that gives us the lowest error rate . # The elbow method plots the error rate of a range of K values error_rate = [] # Set the range of potential K values # Run KNN for each K in the range for i in range(1,32,2): knn = KNeighborsClassifier(n_neighbors = i) fit = knn.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a K value plt.plot(range(1,32,2), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. K Value&#39;) plt.xlabel(&#39;K&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . K value chosen is 7 . Now let&#39;s test the P value for 1 and 2 . error_rate = [] # Set the range of potential P values # Run KNN for each P in the range for i in range(1,3): knn = KNeighborsClassifier(n_neighbors = 7, p = i) fit = knn.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a P value plt.plot(range(1,3), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. P Value&#39;) plt.xlabel(&#39;P&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . P value chosen is 1 (Manhattan Distance) . Build the KNN with the default parameters . knn = KNeighborsClassifier(n_neighbors = 5, # Default 5 weights = &#39;uniform&#39;, # Default uniform metric = &#39;minkowski&#39;, # Default minkowski p = 2 # Default 2 ) # Train the model on the training set fit = knn.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(knn, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 86.22% Cross Validation Accuracy: 85.79% Model Fitting: 0.43% Overfitted Error Rate: 13.78% Confusion Matrix: [[1046 11 0 310] [ 2 2101 0 223] [ 0 0 809 0] [ 241 213 0 2303]] Classification Report: precision recall f1-score support Attacker 0.81 0.77 0.79 1367 Defender 0.90 0.90 0.90 2326 Goalkeeper 1.00 1.00 1.00 809 Midfielder 0.81 0.84 0.82 2757 micro avg 0.86 0.86 0.86 7259 macro avg 0.88 0.88 0.88 7259 weighted avg 0.86 0.86 0.86 7259 . Now build the KNN with the tuned parameters . knn = KNeighborsClassifier(n_neighbors = 7, # Default 5 weights = &#39;uniform&#39;, # Default uniform metric = &#39;minkowski&#39;, # Default minkowski p = 1 # Default 2 ) # Train the model on the training set fit = knn.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(knn, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 86.93% Cross Validation Accuracy: 86.25% Model Fitting: 0.67% Overfitted Error Rate: 13.07% Confusion Matrix: [[1057 9 0 301] [ 3 2111 0 212] [ 0 0 809 0] [ 220 204 0 2333]] Classification Report: precision recall f1-score support Attacker 0.83 0.77 0.80 1367 Defender 0.91 0.91 0.91 2326 Goalkeeper 1.00 1.00 1.00 809 Midfielder 0.82 0.85 0.83 2757 micro avg 0.87 0.87 0.87 7259 macro avg 0.89 0.88 0.88 7259 weighted avg 0.87 0.87 0.87 7259 . Outcome: . Tuning the model improves performance in all areas as seen by the output . KNN 3: 80% Training Set, 20% Test Set . Set the Features, Outcomes and Training/Test Set . feature_cols = [&#39;Crossing&#39;,&#39;Finishing&#39;,&#39;HeadingAccuracy&#39;,&#39;ShortPassing&#39;,&#39;Volleys&#39;,&#39;Dribbling&#39;,&#39;Curve&#39;,&#39;FKAccuracy&#39;, &#39;LongPassing&#39;,&#39;BallControl&#39;,&#39;Acceleration&#39;,&#39;SprintSpeed&#39;,&#39;Agility&#39;,&#39;Reactions&#39;,&#39;Balance&#39;,&#39;ShotPower&#39;, &#39;Jumping&#39;,&#39;Stamina&#39;,&#39;Strength&#39;,&#39;LongShots&#39;,&#39;Aggression&#39;,&#39;Interceptions&#39;,&#39;Positioning&#39;,&#39;Vision&#39;, &#39;Penalties&#39;,&#39;Composure&#39;,&#39;Marking&#39;,&#39;StandingTackle&#39;,&#39;SlidingTackle&#39;] # Assign the feature data X = df[feature_cols] # Assign the outcomes y = df.Position # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1984) . Use the Elbow Method to find the best K . We&#39;re looking for the lowest value that gives us the lowest error rate . # The elbow method plots the error rate of a range of K values error_rate = [] # Set the range of potential K values # Run KNN for each K in the range for i in range(1,32,2): knn = KNeighborsClassifier(n_neighbors = i) fit = knn.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a K value plt.plot(range(1,32,2), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. K Value&#39;) plt.xlabel(&#39;K&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . K value chosen is 7 . Now let&#39;s test the P value for 1 and 2 . error_rate = [] # Set the range of potential P values # Run KNN for each P in the range for i in range(1,3): knn = KNeighborsClassifier(n_neighbors = 7, p = i) fit = knn.fit(X_train, y_train) pred_i = fit.predict(X_test) # Record the error value error_rate.append(np.mean(pred_i != y_test)) # Plot the error rates and choose a P value plt.plot(range(1,3), error_rate, color = &#39;blue&#39;, linestyle = &#39;dashed&#39;, markerfacecolor = &#39;red&#39;, marker = &#39;o&#39;, markersize = 5) plt.title(&#39;Error Rate vs. P Value&#39;) plt.xlabel(&#39;P&#39;) plt.ylabel(&#39;Error Rate&#39;) plt.show() . P value chosen is 2 (Euclidean Distance) . Build the KNN with the default parameters . knn = KNeighborsClassifier(n_neighbors = 5, # Default 5 weights = &#39;uniform&#39;, # Default uniform metric = &#39;minkowski&#39;, # Default minkowski p = 2 # Default 2 ) # Train the model on the training set fit = knn.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(knn, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 86.67% Cross Validation Accuracy: 85.79% Model Fitting: 0.87% Overfitted Error Rate: 13.33% Confusion Matrix: [[ 543 6 0 157] [ 0 1028 0 106] [ 0 0 402 0] [ 113 102 0 1173]] Classification Report: precision recall f1-score support Attacker 0.83 0.77 0.80 706 Defender 0.90 0.91 0.91 1134 Goalkeeper 1.00 1.00 1.00 402 Midfielder 0.82 0.85 0.83 1388 micro avg 0.87 0.87 0.87 3630 macro avg 0.89 0.88 0.88 3630 weighted avg 0.87 0.87 0.87 3630 . Now build the KNN with the tuned parameters . knn = KNeighborsClassifier(n_neighbors = 7, # Default 5 weights = &#39;uniform&#39;, # Default uniform metric = &#39;minkowski&#39;, # Default minkowski p = 2 # Default 2 ) # Train the model on the training set fit = knn.fit(X_train, y_train) # Test the model on the test set y_pred = fit.predict(X_test) # Build a confusion matrix cm = confusion_matrix(y_test, y_pred) # Cross Validate the model with 10 folds cv_scores = cross_val_score(knn, X, y, cv=10) # Set the accuracy to a variable acc = metrics.accuracy_score(y_test, y_pred) # Set the error rate to a variable err = np.mean(y_pred != y_test) # Set the cross validation mean score to a variable cvm = np.mean(cv_scores) # Calculate the differece between the Accuracy and CV Accuracy dif = acc-cvm # Determine if model is underfitted or overfitted if dif &lt; 0: underover = &quot;Underfitted&quot; else: underover = &quot;Overfitted&quot; print &quot;Model Accuracy: &quot;,str(round(acc*100,2))+&quot;%&quot; print &quot;Cross Validation Accuracy: &quot;,str(round(cvm*100,2))+&quot;%&quot; print &quot;Model Fitting: &quot;,str(round(dif*100,2))+&quot;%&quot;,underover print &quot;Error Rate: &quot;,str(round(err*100, 2))+&quot;%&quot; print &quot; n n nConfusion Matrix: n n&quot;,cm print &quot; n n nClassification Report: n n&quot;,classification_report(y_test, y_pred) . Model Accuracy: 87.3% Cross Validation Accuracy: 86.48% Model Fitting: 0.82% Overfitted Error Rate: 12.7% Confusion Matrix: [[ 553 4 0 149] [ 0 1032 0 102] [ 0 0 401 1] [ 104 101 0 1183]] Classification Report: precision recall f1-score support Attacker 0.84 0.78 0.81 706 Defender 0.91 0.91 0.91 1134 Goalkeeper 1.00 1.00 1.00 402 Midfielder 0.82 0.85 0.84 1388 micro avg 0.87 0.87 0.87 3630 macro avg 0.89 0.89 0.89 3630 weighted avg 0.87 0.87 0.87 3630 . Outcome: . Tuning the model improves performance in all areas as seen by the output . KNN Outcome: . The best performing model was the one built on the 80% training set based on the output of each model. . Model Outcome: . The best performing model overall was the KNN model built on the 80% training set based on the output of each model. .",
            "url": "https://4nil.com/2022/04/08/Player-Position-Classification.html",
            "relUrl": "/2022/04/08/Player-Position-Classification.html",
            "date": " • Apr 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://4nil.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://4nil.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://4nil.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://4nil.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}